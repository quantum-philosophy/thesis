In the previous chapters, we have been primarily concerned with making
uncertainty-aware decisions at design- and fabrication-time, that is, during
various development and manufacturing stages of a computer system. In this
chapter, we elaborate on making such decisions at runtime and, more
specifically, on resource management in the present of workload uncertainty.

\section{\introductiontitle}
\slab{network-introduction}
\inputsection{introduction}

\section{\problemtitle}
\slab{network-problem}
\inputsection{problem}

\section{\priortitle}
\slab{network-prior}
\inputsection{prior}

\section{\solutiontitle}
\slab{network-solution}
\inputsection{solution}

\section{\resultstitle}
\slab{network-results}

The infrastructure developed for the experiments presented below is open source
and available online at \cite{eslab2017b}. The implementation is based on
TensorFlow \cite{abadi2015}. The experiments are conducted on a \up{GNU}/Linux
machine equipped with 8 \up{CPU}s Intel Core i7-3770 3.4~\up{GH}z, 16~\up{GB} of
\up{RAM}, and an \up{HDD} of 500~\up{GB}. The machine has no modern \up{GPU}s;
therefore, the reported results have an immense room for improvement.

\hiddensubsection{Data Processing}

Recall that the considered data set is the Google cluster-usage traces
\cite{reiss2011} described in \sref{data}. In the experiments, we focus on one
particular resource ($d = 1$), which is the \up{CPU} usage of the tasks executed
in the cluster. The resource-usage table contains two apposite columns: the
average and maximal \up{CPU} usage over five-minute intervals; we extract the
former.

The grouping and indexing steps of the data-processing pipeline described in
\sref{grouping} and \sref{indexing}, respectively, and depicted in
\fref{workflow} take approximately 57 hours to finish (no parallelism). Since
they have to be done only once, their computational cost can be safely
considered negligible.

Regarding the selection stage delineated in \sref{selection}, we filter those
resource-usage traces that contain 5--50 data points; consequently, $l_i \in [5,
50]$ in \eref{trace}. Such traces constitute around 74\% of the total number of
traces (around 18 out of 24 million). We experiment with a random subset of two
million traces, which is around 11\% of the 5--50 resource-usage traces; hence,
$n = 2 \times 10^6$ in \eref{traces}. The data sets $X_1$, $X_2$, and $X_3$
constitute 70\%, 10\%, and 20\% of $X$, respectively. Fetching and storing on
disk these many traces take approximately four hours. Recall that this operation
has to be repeated only when the selection criteria change, which happens rarely
in practice.

\hiddensubsection{Learning}

The training stage (see \sref{training} and recall \fref{workflow}) is
configured as follows. Ten buckets or, equivalently, queues are used according
to the following rule: $l < 6 < 7 < 8 < 9 < 10 < 15 < 20 < 30 < 40 \leq 50$. The
batch size $b$ is set to 64. The optimization algorithm that is employed for
minimizing the loss function is Adam \cite{kingma2014}, which is an adaptive
technique. The algorithm is applied with its default settings.

Regarding the validation stage, the considered hyperparameters are the number of
cells $c$ (the blue boxes in \fref{model}), number of units per cell $u$ (the
double circles in \fref{model}), and probability of dropout $p$ as discussed in
\sref{validation}. More concretely, we let $c \in \{1, 2, 3, 4, 5\}$, $u \in
\{100, 200, 400, 800, 1600\}$, and $p \in \{0, 0.25, 0.5\}$, which yields 75
different combinations in total. The candidates are explored by means of the
Hyperband algorithm introduced in \sref{validation} with its default settings.
The maximum budget for one configuration is set to four training epochs, which
correspond to $4 \times 0.7 \times 2 \times 10^6 = 5.6 \times 10^6$
resource-usage traces or $5.6 \times 10^6 \div 64 = 87\,500$ training steps.

The above exploration, which encompasses both the training and validation
stages, takes approximately 15 days to finish. During this process, we run up to
four training sessions in parallel, which typically keeps all the eight
\up{CPU}s busy. It should be noted that, since the training, validation, and
testing data sets have been cached on disk as a result of our data-processing
pipeline described in \sref{data}, individual training sessions do not have any
overhead in this regard.

The results of the validation stage are given in \tref{validation}, which shows
the mean squared error (\up{MSE}) of the top 10 configurations of the
hyperparameters as measured using $X_2$ ($0.1 \times 2 \times 10^6 = 2 \times
10^5$ traces). The best trained model is found to have the following
hyperparameters: $c = 3$, $u = 1600$, and $p = 0$. In general, deeper and wider
architectures tend to outperform shallower and narrower ones (the depth and
breadth are measured by $c$ and $u$, respectively), which is expected. In these
experiments, the dropout mechanism does not have much impact on the resulting
accuracy, which is likely due to the amount of data being enough for
regularizing the model.

\tref{validation} also shows an estimate of the memory required by each
configuration, including the model's trainable parameters and internal state. It
can be seen that, if the memory usage is a concern, one could trade a small
decrease in accuracy for a considerable memory saving. For example, the fourth
best configuration requires 75\% less memory than the first one.

After the exploration stage, the best trained model is taken to the testing
stage (see \sref{testing}), which is undertaken using $X_3$ ($0.2 \times 2
\times 10^6 = 4 \times 10^5$ traces). At this stage, the model is extensively
assessed by predicting the resource usage of individual tasks multiple time
steps ahead at each step of the testing traces in $X_3$. In these experiments,
we predict four steps into the future ($h = 4$ in \sref{problem}). This
elaborate sequential testing procedure takes around 18 hours from start to
finish.

In order to assess better the accuracy of our model, we employ also an
alternative one, which we shall refer to as the reference model. The reference
model is based on random walk. It postulates that the best prediction of what
will happen tomorrow is what happens today plus an optional random offset, which
we set to zero. In other words, the next value of a resource-usage trace is
estimated to be the current one, which results in four identical predictions at
each time step.

The results of the testing stage can be seen in \fref{testing}, which shows the
\up{MSE} of our model (the blue line) as well as the one of the reference model
(the orange line) with respect to $X_3$. The magnitude of our model's errors
suggests that the amount of regularity present in the data is not sufficient to
make the resource-usage predictions highly accurate. Nevertheless, it can be
seen in \fref{testing} that, relative to the reference model, our model provides
an error reduction of approximately 47\% at each of the four future time
moments. This observation indicates that a certain structure does exist, and
that it can be identified and utilized in order to make educated predictions.

\section{\conclusiontitle}
\slab{network-conclusion}

We have presented our experience of working with the state-of-the-art recurrent
neural networks in the context of fine-grained long-range prediction of the
resource usage in a computer cluster. Our workflow---which starts from making
the data readily available for learning and finishes by predicting the resource
usage of individual tasks multiple time steps into the future---has been
described in detail and applied to a large data set of real resource-usage
traces of a computer cluster.

The experimental results suggest that the considered fine-grained traces possess
a certain structure, and that this structure can be extracted by advanced
machine-learning techniques and subsequently utilized for making educated
predictions. This information can be of use to such a crucial component as the
resource manager of the computer cluster in question, allowing the manager to
more intelligently orchestrate the cluster.

In order to facilitate the development of intelligent resource managers of
computer clusters, we investigate the utility of the state-of-the-art neural
networks for the purpose of fine-grained long-range prediction of the resource
usage in one such cluster. We consider a large data set of real-life traces and
describe in detail our workflow, starting from making the data accessible for
learning and finishing by predicting the resource usage of individual tasks
multiple steps ahead. The experimental results indicate that such fine-grained
traces as the ones considered possess a certain structure, and that this
structure can be extracted by advanced machine-learning techniques and
subsequently utilized for making informed predictions.
